<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Transformer on XFious</title>
        <link>https://example.org/tags/transformer/</link>
        <description>Recent content in Transformer on XFious</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <managingEditor>xfious@outlook.com (XFious)</managingEditor>
        <webMaster>xfious@outlook.com (XFious)</webMaster>
        <lastBuildDate>Fri, 24 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://example.org/tags/transformer/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[Paper Note] Language Models are Unsupervised Multitask Learners</title>
        <link>https://example.org/machine-learning/paper-note/gpt2/</link>
        <pubDate>Fri, 24 Nov 2023 00:00:00 +0000</pubDate>
        <author>xfious@outlook.com (XFious)</author>
        <guid>https://example.org/machine-learning/paper-note/gpt2/</guid>
        <description>&lt;img src="https://example.org/machine-learning/paper-note/gpt2/gpt2-size.png" alt="Featured image of post [Paper Note] Language Models are Unsupervised Multitask Learners" /&gt;&lt;h1 id=&#34;key-insights&#34;&gt;Key Insights&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;This paper presents a general purpose training procedure that can be applied to a variety of NLP tasks, using task instructions and task input as conditioning factors.&lt;/li&gt;
&lt;li&gt;A model trained with a massive, diverse, and unsupervised dataset can handle many tasks in a zero-shot manner and typically outperforms SOTA task-specific models.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;problems-of-previous-methods&#34;&gt;Problems of Previous Methods&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Heavily supervised methods require large amounts of labeled data; however, some tasks lack high-quality labeled data or a sufficient volume of it.&lt;/li&gt;
&lt;li&gt;Previously, different training objectives needed to be formulated for each task.&lt;/li&gt;
&lt;li&gt;As training was specific to a certain task and narrow dataset, the model had limited generalization and transfer capabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;advantages-of-this-method&#34;&gt;Advantages of This Method&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The model utilizes self-supervised learning, eliminating the need for labeled data.&lt;/li&gt;
&lt;li&gt;The training procedure and objective are versatile, applicable to both pre-training and downstream tasks.&lt;/li&gt;
&lt;li&gt;It can handle many tasks in a zero-shot approach.&lt;/li&gt;
&lt;li&gt;No changes to the model&amp;rsquo;s architecture are required.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;approachcore-idea&#34;&gt;Approach/Core Idea&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;This paper treats language modeling as a conditional generation problem. It takes advantage of language&amp;rsquo;s inherent sequential order: subsequent tokens are conditioned on previous ones. The goal is to maximize the sequence&amp;rsquo;s probability.&lt;/li&gt;
&lt;li&gt;$P(x) = \prod_{i=1}^n P(x_i|x_{&amp;lt;i})$&lt;/li&gt;
&lt;li&gt;Text generation is conditioned on the task instruction and task input, both presented in text format.&lt;/li&gt;
&lt;li&gt;The training corpus must be diverse enough to encompass language distribution and large enough to provide sufficient training data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;method&#34;&gt;Method&lt;/h1&gt;
&lt;h2 id=&#34;about-input&#34;&gt;About Input&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The necessary corpus characteristics include:
&lt;ul&gt;
&lt;li&gt;Diversity: The corpus should not be limited to just dialogues, news, or novels.&lt;/li&gt;
&lt;li&gt;Logical coherence: The text should reflect the underlying rules of the language. Common crawl data has too much noise prior to filtering.&lt;/li&gt;
&lt;li&gt;Volume and ease of collection: The corpus should be both extensive and easily collectable.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;WebText: This paper uses links in Reddit as a data source. As these links are user-selected, they offer high quality at a low cost.&lt;/li&gt;
&lt;li&gt;Wikipedia is kept separate to avoid duplication.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;encoding-strategy-byte-pair-encoding-bpe&#34;&gt;Encoding Strategy: Byte Pair Encoding (BPE)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What is BPE:
&lt;ul&gt;
&lt;li&gt;To establish a vocabulary table for tokenization, the BPE algorithm treats text as a byte sequence and merges byte sub-sequences based on frequency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Advantages of BPE:
&lt;ul&gt;
&lt;li&gt;It can encompass all strings, encompassing unknown and artificial words.&lt;/li&gt;
&lt;li&gt;It doesn&amp;rsquo;t require lossy preprocessing like casting all words to lowercase.&lt;/li&gt;
&lt;li&gt;This method strikes a balance between vocabulary size and word fragmentation. If Unicode isn&amp;rsquo;t broken down, the vocabulary size becomes excessively large due to Unicode&amp;rsquo;s vast range. If only 256 bytes are used to form any string without merging them into vocabulary, many semantics embedded in words will be lost, significantly increasing learning difficulty.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Problem with naive BPE:
&lt;ul&gt;
&lt;li&gt;It merges too many variants of common words like &amp;ldquo;dog,&amp;rdquo; &amp;ldquo;dogs,&amp;rdquo; &amp;ldquo;dog. dog!&amp;rdquo; and &amp;ldquo;dog?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;This approach wastes vocabulary space and model capacity.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The solution involves categorizing strings into different types. For instance, punctuation can be separated from words. Strings cannot be merged across different categories unless it&amp;rsquo;s a space, which improves compression efficiency.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A decoder-only transformer serves as the backbone.&lt;/li&gt;
&lt;li&gt;Pre-norm residual blocks are employed.&lt;/li&gt;
&lt;li&gt;The weights in the transformer block are initialized with 1/sqrt(N), where N is the number of layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;training-process-and-loss-function&#34;&gt;Training Process and Loss Function&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The learning rate is tuned using 5% of the WebText dataset.&lt;/li&gt;
&lt;li&gt;Four models are trained with a geometrically increasing number of parameters.&lt;/li&gt;
&lt;li&gt;Sequence length (seqlen) is set to 1024.&lt;/li&gt;
&lt;li&gt;Batch size is set to 512.&lt;/li&gt;
&lt;li&gt;Loss function: $L = \sum_{i=1}^n \frac{cross\underline{\hspace{0.5em}}entropy(p_i, x_i)}{seqlen}$
&lt;ul&gt;
&lt;li&gt;Here, p represents the probability of the predicted token, and x denotes the ground truth token.&lt;/li&gt;
&lt;li&gt;Spanning from the first to the last token, the loss is computed as the average cross entropy between the predicted token and ground truth token.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;metrics-and-evaluation&#34;&gt;Metrics and Evaluation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Perplexity (PPL): Lower values are better. PPL is calculated as $PPL = \exp(L)$&lt;/li&gt;
&lt;li&gt;Bits per byte (BPB): This measures the model&amp;rsquo;s compression efficiency. Lower values are better. BPB is calculated as $BPB = \frac{L}{log(2)}$&lt;/li&gt;
&lt;li&gt;Bits per character (BPC): Lower values are preferable.&lt;/li&gt;
&lt;li&gt;Accuracy (ACC): Higher values are better. This metric is used for multiple-choice tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;results-and-findings&#34;&gt;Results and Findings&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The model can handle many tasks across different domains in a zero-shot manner, demonstrating considerable improvement on tasks with small datasets.&lt;/li&gt;
&lt;li&gt;The model performs reliably on noisy data and out-of-distribution data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;childrens-book-test&#34;&gt;Childrenâ€™s Book Test&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This dataset tests the model&amp;rsquo;s ability to discern the missing word with a context of 20 sentences provided for each question. The model should capture the language&amp;rsquo;s long-term dependencies.&lt;/li&gt;
&lt;li&gt;Even the smallest model surpasses the state-of-the-art (SOTA).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lambada&#34;&gt;LAMBADA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The task involves predicting the last word of a sentence. For humans, approximately 50 words are typically needed to predict the last word.&lt;/li&gt;
&lt;li&gt;The models outperform the SOTA in terms of perplexity (PPL).&lt;/li&gt;
&lt;li&gt;For incorrect predictions, most predictions are valid continuations of the sentence but not valid final words. This suggests that the models are not specialized for this task and do not know they should predict the sentence&amp;rsquo;s last word.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;coqa&#34;&gt;CoQA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This task tests reading comprehension, and the model needs to understand the conversation history because it includes questions like &amp;ldquo;why&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;The model surpasses the SOTA without fine-tuning.&lt;/li&gt;
&lt;li&gt;Problem and finding: The model often uses simple retrieval-based heuristics, such as directly copying a name from the context to answer a &amp;ldquo;who&amp;rdquo; question.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;summarization&#34;&gt;Summarization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The prompt &amp;ldquo;TL;DR:&amp;rdquo; is used to induce summarization.&lt;/li&gt;
&lt;li&gt;The performance falls short of classic neural baselines.&lt;/li&gt;
&lt;li&gt;Problem and finding:
&lt;ul&gt;
&lt;li&gt;Capturing long-distance dependencies is challenging. The model tends to summarize only the most recent sentences.&lt;/li&gt;
&lt;li&gt;It makes mistakes on details, such as how many cars were involved in a crash.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;translation&#34;&gt;Translation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Although English predominates in the training corpus, the model shows limited ability to understand and translate French. The volume of French data is 500 times smaller than typical monolingual translation datasets.&lt;/li&gt;
&lt;li&gt;The overall performance significantly trails behind SOTA.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;question-answering&#34;&gt;Question Answering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Factoid questions were used to assess and quantify how much knowledge has been learned by the model.&lt;/li&gt;
&lt;li&gt;Model size is crucial for performance.&lt;/li&gt;
&lt;li&gt;Compared with an open domain question answering system that uses a retrieval-based method, the largest GPT2 performs significantly worse.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;inspiration-and-derivatives-from-this-paper&#34;&gt;Inspiration and Derivatives from This Paper&lt;/h1&gt;
&lt;h2 id=&#34;distribution-of-language&#34;&gt;Distribution of Language&lt;/h2&gt;
&lt;p&gt;An effective language model should be capable of learning the inherent rules of language, as these rules determine the distribution of language. By evaluating what factors influence language and whether these factors can be learned easily, we can identify the model&amp;rsquo;s weaknesses and areas for improvement.&lt;/p&gt;
&lt;p&gt;Language, an outcome of thought, conceptualizes human senses and cognition, serving as a medium for communication and a vehicle for thought. It can also be viewed as an agent&amp;rsquo;s reaction to its environment, functioning as an explicit symbol for conveying messages or abstracting entities to facilitate logical reasoning. However, since language compresses information and cannot fully represent its referent, it cannot fully reflect the process of thought but rather serves as an output of this process.&lt;/p&gt;
&lt;p&gt;From a text-generation perspective, each subsequent token is conditioned by preceding text. The relationship between the last token and preceding text can be syntactical or semantic. Among these, semantic relationships are the most crucial and challenging to learn. Semantics originate from the agent&amp;rsquo;s cognitive activity and are influenced by various factors that may be hard to infer from preceding text. By examining these factors and tracking their influence on cognition, we can identify which elements are difficult for models to predict, thereby pinpointing the model&amp;rsquo;s weaknesses and potential areas for enhancement.&lt;/p&gt;
&lt;p&gt;Factors that are challenging to predict typically exhibit the following characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Strong associations with known conditions (preceding text), such that a high-probability answer can be inferred from previous context. For instance, common sense is easily predictable; if during text generation the given statement is &amp;ldquo;The sky is&amp;hellip;&amp;rdquo;, we can confidently predict &amp;ldquo;blue&amp;rdquo; as the next word. However, personal experiences are unpredictable; if given &amp;ldquo;When I was a child&amp;hellip;&amp;rdquo;, predicting the next word becomes challenging due to the unique nature of individual experiences.&lt;/li&gt;
&lt;li&gt;For unknown factors not directly related to previous context but indirectly influencing future text through the state of the agent, predictability becomes challenging.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Factors that might influence current semantics include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Preceding text: This factor is easy for a well-trained model to predict as the meaning can typically be inferred from the context.&lt;/li&gt;
&lt;li&gt;Environment: This represents the agent&amp;rsquo;s physical and social surroundings, which might not be explicitly stated in the text.
&lt;ul&gt;
&lt;li&gt;Context: This can usually be inferred to a large degree from preceding text and can significantly influence subsequent text. However, it may not always be fully embodied in preceding text.&lt;/li&gt;
&lt;li&gt;Agent&amp;rsquo;s environment: Very little about this can be inferred from preceding text. Although it might not directly impact preceding text, it indirectly influences future text by affecting the agent&amp;rsquo;s state.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Memory
&lt;ul&gt;
&lt;li&gt;Common knowledge and cultural background: These are easily predictable; given preceding context, we can confidently infer what common knowledge is used. This directly impacts future text in a similar manner as preceding context.&lt;/li&gt;
&lt;li&gt;Personal experiences: These are hard to predict unless explicitly mentioned in preceding text. Like preceding context, personal experiences directly influence future text.&lt;/li&gt;
&lt;li&gt;Domain knowledge: This is relatively easy to predict by determining a field or domain of knowledge through the context and preceding text. The challenge here lies in small-scale models that do not have enough capacity to accommodate this low-frequency knowledge.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Inner state refers to intermediate products of the cognitive process, rather than the preceding text itself. This does not aim to describe the entire cognitive process but rather views it as a black box, assuming intermediate outputs for better understanding of the model.
&lt;ul&gt;
&lt;li&gt;Perception and understanding of environment: These are hard to infer but more direct than environmental factors.&lt;/li&gt;
&lt;li&gt;Thoughts and understanding of preceding text: These are relatively easier to infer since text is an output of thought. The only challenging part is predicting a complete thought or cognitive process; however, they significantly influence future text.&lt;/li&gt;
&lt;li&gt;Motivation: This is relatively hard to predict. Although most people will have similar motivations under similar circumstances, the environment is unpredictable, making motivations equally hard to predict. Motivations indirectly influence future text by affecting thoughts and emotions.&lt;/li&gt;
&lt;li&gt;Emotions: These are usually reflected in text and thus easier to infer; they directly influence future text quite significantly.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This analysis indicates that even though large language models display impressive language comprehension abilities, there are still factors difficult to infer from preceding text. By providing additional information, we could potentially enhance the model&amp;rsquo;s understanding of language, particularly the underlying generation process, thereby improving its overall performance.&lt;/p&gt;
&lt;h2 id=&#34;why-does-this-training-procedure-work&#34;&gt;Why does this training procedure work?&lt;/h2&gt;
&lt;p&gt;A key advantage of this method is that for each token, the input&amp;rsquo;s distribution is similar; this means that any substring within a piece of text, designated as $p(x_i, x_{&amp;lt;i})$, falls within the training distribution. Although training is conducted in parallel, attention masking ensures that the model only sees preceding tokens and not future ones, thereby aligning the distributions during the training and inference phases.&lt;/p&gt;
&lt;p&gt;A sufficiently challenging objective is also crucial as it compels the model to learn the intrinsic rules influencing language distribution. If the training data covers a wide range, the model can&amp;rsquo;t merely overfit a subset of data through shortcuts; instead, it must predict all data by deciphering these inherent rules, thereby encompassing a vast body of knowledge. However, since the model can only indirectly learn about inner states, its understanding cannot fully represent cognitive states, resulting in limited logical and inferential capabilities. To render text more regular, the content should ideally encompass comprehensive information; for reference on this aspect, we can refer back to the analysis on language distribution. Despite its challenging nature, this objective is achievable by leveraging Transformers&amp;rsquo; capability to capture long-distance dependencies, allowing model convergence.&lt;/p&gt;
&lt;h2 id=&#34;why-the-model-can-zero-shot-many-tasks&#34;&gt;Why the model can zero-shot many tasks?&lt;/h2&gt;
&lt;p&gt;Zero-shot refers to a modelâ€™s ability to perform reasonably well on a task without having been specifically trained for it. The objective of this model is to predict subsequent content based on preceding text, thereby creating a unified prediction format. This format can effectively guide the model&amp;rsquo;s behavior through prompts, harmonizing the way context and tasks are conditioned. Given sufficient conditions, the model can understand the task at hand. The model&amp;rsquo;s ability to not only comprehend tasks but also accomplish them stems from its understanding of the underlying rules of language. Essentially, it learns these deeper determinants of language distribution from extensive corpora; viewed from this deeper level, downstream tasks do not exceed the scope of the training distribution.&lt;/p&gt;
&lt;h2 id=&#34;why-has-this-level-of-intelligence-emerged-in-natural-language-modalities-while-video-based-auto-regressive-models-havent-reached-this-level-yet&#34;&gt;Why has this level of intelligence emerged in natural language modalities, while video-based auto-regressive models haven&amp;rsquo;t reached this level yet?&lt;/h2&gt;
&lt;p&gt;Language is a product of the thought process, serving as a vehicle for thought and a means of expressing dense semantics. Therefore, it is closer to human-understood intelligence compared to imagery. For video-based auto-regressive models, learning the laws of the world from videos and gaining an understanding of these global change patterns through a similar self-supervised approach is a more challenging task. Language inherently sets a context; even scattered text on the internet can construct context to some extent, although video can also achieve this.&lt;/p&gt;
&lt;p&gt;Finally, from a hardware resource perspective, language has high information density; a small amount of text can convey substantial information. In contrast, imagery requires numerous pixels to express comparable information, making the data throughput requirements for language lower than visual modalities.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>preliminary experiment for LLM distillation and pretraining</title>
        <link>https://example.org/project/dearth-tiny/</link>
        <pubDate>Mon, 13 Nov 2023 00:00:00 +0000</pubDate>
        <author>xfious@outlook.com (XFious)</author>
        <guid>https://example.org/project/dearth-tiny/</guid>
        <description>&lt;img src="https://example.org/project/dearth-tiny/img.png" alt="Featured image of post preliminary experiment for LLM distillation and pretraining" /&gt;&lt;h1 id=&#34;language-model-on-tinystories&#34;&gt;Language Model on TinyStories&lt;/h1&gt;
&lt;iframe src=&#34;https://xfious-dearth-tiny.hf.space&#34; style=&#34;width: calc(100%); height: calc(100vh - 100px); margin: 0px&#34;&gt;&lt;/iframe&gt;
&lt;h1 id=&#34;about-this-project&#34;&gt;About this project&lt;/h1&gt;
&lt;h2 id=&#34;what-is-this-project&#34;&gt;What is this project?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This is a preliminary experiment for pretraining language model, and using distillation to accelerate training and improve performance. The experiment is to verify the effectiveness of the following method:
&lt;ul&gt;
&lt;li&gt;DeepNet (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2203.00555.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2203.00555.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Distillation framework, and corresponding loss function (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2002.10957.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2002.10957.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;New Optimizer Sophia (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2305.14342.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2305.14342.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;front window and attention window, LM_infinite (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2308.16137.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2308.16137.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;other hyperparameters of the training process.&lt;/li&gt;
&lt;li&gt;using MSE loss for soft label&lt;/li&gt;
&lt;li&gt;group-query attention (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2305.13245.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2305.13245.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;computation budget (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2203.15556.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2203.15556.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dearth-tiny is a LM trained based on TinyStories. This model can write short stories with children&amp;rsquo;s level vocabulary; it is not used for instruction QA.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-the-significance-and-purpose-of-this-project&#34;&gt;What is the significance and purpose of this project?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Obtain smaller models through distillation that can be run on CPU and mobile devices&lt;/li&gt;
&lt;li&gt;Compared with pruning methods like Sheared LLaMA, distillation allows more flexible model structure&lt;/li&gt;
&lt;li&gt;Compared with direct training, distillation may improve the effect and accelerate training&lt;/li&gt;
&lt;li&gt;Make the model deeper and have more layers, improving the effect to a certain extent&lt;/li&gt;
&lt;li&gt;Trying to make the model handle extremely long sequences&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-the-limitation-of-this-project&#34;&gt;What is the limitation of this project?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Due to the limitation of training data, this model can only generate short stories using very simple words, which means most language is out of distribution.&lt;/li&gt;
&lt;li&gt;The purpose of the TinyStories dataset is to serve as a sanity check, verifying the effect of the model and training process; the scope and knowledge of the data are very limited, so it does not require a large number of parameters to show a good performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;the-distillation-and-training-process&#34;&gt;The distillation and training process&lt;/h1&gt;
&lt;h2 id=&#34;about-the-model-structure&#34;&gt;About the model structure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;DeepNet: Amplify the residual connection so that the gradient of the layer close to the input will not be too small; allow a deeper model structure to improve the effect to a certain extent;&lt;/li&gt;
&lt;li&gt;LM_infinite: using the front attention window to make later tokens can attend to the front token with enough weights, preventing the out-of-distribution problem.&lt;/li&gt;
&lt;li&gt;Mistral: using attention window to solve long sequence problem (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2310.06825.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2310.06825.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Adding rotary position embedding to query and key vectors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-to-distill-and-train-the-model&#34;&gt;How to distill and train the model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;One training objective is to imitate the attention map and value, and the other is to imitate the logits.&lt;/li&gt;
&lt;li&gt;Using MSE loss for soft label and comparing logits between teacher and student, because according to the profiler&amp;rsquo;s result, softmax required by KL divergence is very time-consuming.&lt;/li&gt;
&lt;li&gt;Seqence length = 256, batch_size for distillation = 200, batch_size for training = 800&lt;/li&gt;
&lt;li&gt;2k steps for distillation with 300 steps warmup, 2k steps for training.&lt;/li&gt;
&lt;li&gt;Using Sophia optimizer, peak lr = 5e-4, beta1 = 0.9, beta2 = 0.99, weight decay = 0.2.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-the-error-that-has-been-corrected&#34;&gt;What is the error that has been corrected?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In the previous project about low-rank LM, the loss function is wrong, because it only checks the logits of the last token, causing a very slow and unstable training process. It is unreasonable to only check the last token&amp;rsquo;s logits, because the model is trained to predict the next token, not the last token, so every output is usable to estimate the loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.org/project/dearth-tiny/loss-ts100-re2.png&#34;
	width=&#34;977&#34;
	height=&#34;990&#34;
	srcset=&#34;https://example.org/project/dearth-tiny/loss-ts100-re2_hufd0b3d3930ee9759a967dbdea3dadaaa_55644_480x0_resize_box_3.png 480w, https://example.org/project/dearth-tiny/loss-ts100-re2_hufd0b3d3930ee9759a967dbdea3dadaaa_55644_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;98&#34;
		data-flex-basis=&#34;236px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: shows each loss component. After 2000 steps, loss_soft and loss_mimic are not used in the training process.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.org/project/dearth-tiny/lr-ts100-re2.png&#34;
	width=&#34;573&#34;
	height=&#34;413&#34;
	srcset=&#34;https://example.org/project/dearth-tiny/lr-ts100-re2_hu1efa4cf25de4292373e54e67f5efcd5b_22492_480x0_resize_box_3.png 480w, https://example.org/project/dearth-tiny/lr-ts100-re2_hu1efa4cf25de4292373e54e67f5efcd5b_22492_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;138&#34;
		data-flex-basis=&#34;332px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2: learning rate for distillation and training.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;work-in-progress&#34;&gt;Work in progress&lt;/h1&gt;
&lt;h2 id=&#34;what-can-be-done-to-improve-the-effect-why-is-the-effect-not-good&#34;&gt;What can be done to improve the effect? Why is the effect not good?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The current PPL of the student model is 1.7, and the teacher PPL is 0.9.&lt;/li&gt;
&lt;li&gt;Need more training steps&lt;/li&gt;
&lt;li&gt;Need to experiment with more suitable learning rate with the new optimizer&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;potential-problems&#34;&gt;Potential problems&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Compared with the teacher model, the student model has fewer parameters. If the student spend too much capacity on fitting the internal structure, it may be difficult to fit the hard label loss (the loss for the next token).&lt;/li&gt;
&lt;li&gt;Whether the distillation process, that is, learning the internal structure, can replace the fact that the model has experienced a large number of tokens; that is to say, is the ability of the large language model derived from the acquisition of attention map, or from the subtle internal representation that must learn from diverse and numerous training data?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-the-work-in-progress&#34;&gt;What is the work in progress?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Adjust the learning rate so that the training process can be intervened manually to adjust; avoid interrupting the training.&lt;/li&gt;
&lt;li&gt;Distill 7B into 1B, and then compare it with other 1B models&lt;/li&gt;
&lt;li&gt;Instruction fine-tuning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-the-future-work&#34;&gt;What is the future work?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;PPO for alignment.&lt;/li&gt;
&lt;li&gt;Memory-augmented model, adding memory to improve the effect of the model; the memory stored in the database can make the model more controllable and may reduce the hallucination caused by the memory stored in the parameters.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Are Small Language Models Low-rank?</title>
        <link>https://example.org/project/low-rank-lm/</link>
        <pubDate>Thu, 15 Jun 2023 00:00:00 +0000</pubDate>
        <author>xfious@outlook.com (XFious)</author>
        <guid>https://example.org/project/low-rank-lm/</guid>
        <description>&lt;img src="https://example.org/project/low-rank-lm/head.png" alt="Featured image of post Are Small Language Models Low-rank?" /&gt;&lt;h1 id=&#34;are-small-language-models-low-rank&#34;&gt;Are Small Language Models Low-rank?&lt;/h1&gt;
&lt;h2 id=&#34;explore-causal-lm-training-and-increase-hidden-dimension-with-low-rank-matrices&#34;&gt;Explore causal LM training and increase hidden dimension with low-rank matrices.&lt;/h2&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Over-parameterized language models are low rank intrinsically. In this project, I trained 2 causal language models with 28M parameters each, such that one is the baseline and another uses low-rank weights but has higher hidden dimensions, and compare their training speed and accuracy. Although they are trained with more than 12 billion tokens, both models fail to finish training. According to the loss curve on hand, the baseline model trounces the low-rank model in speed and performance.
GITHUB: &lt;a class=&#34;link&#34; href=&#34;https://github.com/XiaonanFu-ucsd/COGS185-final-project&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/XiaonanFu-ucsd/COGS185-final-project&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;Large language models show their power in few-shot learning ability, and the potential to solve most downstream tasks. Just like their name, they are large and expensive to train due to the amount of parameters, and hard to infer on a local machine. If there is some method to make small enough models have decent text generate ability, then a personal device can do local inference in a private and efficient manner. Given that the causal language model with high hidden dimension usually performs better, I explored to use low rank matrices product to simulate a larger model with the same amount of parameters. To compare the overall performance of the normal model and low rank model in terms of training efficiency and accuracy, I trained two models with 28M parameters each, with multiple datasets over 7GB of text. In the training process, I faced several challenges and tried to adjust multiple hyper-parameters to speed up training, such as sequence length, batch size, loss function, and learning rate. Although the low rank model does not work in the limited training time, the exploration of training a language model is still a valuable experience.&lt;/p&gt;
&lt;h1 id=&#34;2-related-work&#34;&gt;2. Related Work&lt;/h1&gt;
&lt;h2 id=&#34;21-attention-in-transformers&#34;&gt;2.1 Attention in Transformers&lt;/h2&gt;
&lt;p&gt;Each Attention layer in transformers uses three matrices to map every token embedding to be query, key, and value vectors. The dot product of the query and key vectors, decides the weight of the corresponding value vectors after the SoftMax operation. After this process, one token embedding obtains other embeddingâ€™s value with different weights, so the model has the ability to learn the sequential relationship across different tokens.&lt;/p&gt;
&lt;p&gt;The positional encoding is also critical for the attention mechanism, since the token embedding itself does not contain the structural information, it should be added in every attention layer. There are many ways to add positional encoding, and in the paper &lt;em&gt;Attention Is All You Need&lt;/em&gt;, it presents sinusoidal positional encoding and learned positional encoding, and they find that they have similar performance, but the sinusoidal version may be better at extrapolation.&lt;/p&gt;
&lt;h2 id=&#34;22-causal-language-model&#34;&gt;2.2 Causal Language Model&lt;/h2&gt;
&lt;p&gt;Causal language models use masked attention to restrict every token can only reference the previous token, so they are more relevant and fit the text generation task. In the implementation, they add a mask with some negative infinity to the dot product result, then the number will be zero after SoftMax. Although they are decoder-only structure, the decoder just contains one attention layer and one multilayer feed-forward network.&lt;/p&gt;
&lt;p&gt;From a holistic perspective to describe the text generation process, a tokenizer encode the prompt to a sequence of token, which can be understood as an integer index in a dictionary. The model uses a lookup table to find the embedding for each token, and send embeddings to decoder layers. In the end, embeddings multiply with a large matric to produce the logit, then use SoftMax to find which logit has the largest probability. It is a multiclass classification task.&lt;/p&gt;
&lt;h2 id=&#34;23-low-rank-adaptation&#34;&gt;2.3 Low-rank adaptation&lt;/h2&gt;
&lt;p&gt;Low-rank adaptation is a common fine-turn method to make a base model have a better performance in a downstream task. Inspired by the finding that a large model may not fully utilize their express ability, and those weights have low rank intrinsically, Edward Hu, et al. hypothesize that the amount of weight change also has this low rank property. Based on LORA, fine-turning only need a small amount of parameter to adjust a large model; a larger matrix is the product of two smaller matrices with a much lower rank.&lt;/p&gt;
&lt;img src=&#34;img1.png&#34; width=&#34;200&#34; height=&#34;200&#34; /&gt;
&lt;h2 id=&#34;24-tinystories-dataset&#34;&gt;2.4 TinyStories Dataset&lt;/h2&gt;
&lt;p&gt;Ronen Eldan, et al. build a small enough dataset that covers diverse vocabulary, facts, and reasoning. They used GPT4 prompted with randomly combined none and verb, to produce millions of stories. Using this dataset, their model with 28 million parameters performance better than the GPT2-large with 774 million parameters in terms of story continuation.&lt;/p&gt;
&lt;h1 id=&#34;3-method&#34;&gt;3. Method&lt;/h1&gt;
&lt;p&gt;The datasets are mainly composed of the TinyStories and TinyStories Instruction datasets. I also use Alpaca Chinese, Dolly 15K, Lightnovels_EN, Chinese Metaphors datasets as supplementary, and allow the model learn bilingual text generation. About 2/3 of the data comes from TinyStories datasets.&lt;/p&gt;
&lt;p&gt;Since the datasets are large, I designed a multi-processing dataloader, which can randomly pick which dataset to sample from with the given proportion, and does not need to load all the datasets into the memory. The dataloader needs to tokenize the text while training. If one independent sample text is too long, it will randomly select part of it to fit the specified token sequence length. If the text is too short, it will pad the text with a padding token at the front. The sequence length used in this project is 1024, such that the model needs to predict the last token. By default, the tokenizer will put BOS at the beginning and EOS at the end. To prevent most rows end in EOS, the dataloader will randomly crop the row; it will also increase the robustness of the model. The batch size is 120.&lt;/p&gt;
&lt;p&gt;I used the tokenizer from the Chinese-LLaMA-Alpaca project, which extends the LLaMA vocabulary with Chinese words. The vocabulary size of this tokenizer is 49954.&lt;/p&gt;
&lt;p&gt;The baseline model references the implementation of nanoGPT and Metaâ€™s LLAMA. It has 4 layers with a hidden size of 768 and 12 heads, in a total of 28M parameters, except for the final linear layer which converts embedding to logit. Every attention layer needs positional information. Rather than the methods presented in Attention Is All You Need, this model used rotary positional encoding, which does not add to the token embedding, but element-wisely multiplies to the matrix x_q and x_k. This encoding has decreasing sinusoidal frequency for larger dimensions (Su, et al.). Flash attention is used to speed up attention computation. After an attention layer, there is a two-layer FFN, for each is 768*3072; between them is a SiLU activation function.&lt;/p&gt;
&lt;p&gt;The low rank model has the same structure, but uses low rank weights matrices to achieve a higher hidden dimension, which is 1152, and has 18 heads.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;wq&lt;/th&gt;
&lt;th&gt;wk&lt;/th&gt;
&lt;th&gt;wv&lt;/th&gt;
&lt;th&gt;wo&lt;/th&gt;
&lt;th&gt;Linear&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Baseline&lt;/td&gt;
&lt;td&gt;768*768&lt;/td&gt;
&lt;td&gt;768*768&lt;/td&gt;
&lt;td&gt;768*768&lt;/td&gt;
&lt;td&gt;768*768&lt;/td&gt;
&lt;td&gt;768*3072 * 2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Low-rank&lt;/td&gt;
&lt;td&gt;1152*256 * 2&lt;/td&gt;
&lt;td&gt;1152*256 * 2&lt;/td&gt;
&lt;td&gt;1152*256 * 2&lt;/td&gt;
&lt;td&gt;1152*256 * 2&lt;/td&gt;
&lt;td&gt;(1152*512 + 512*2880) * 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Table 1: comparing the dimension of models&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For both models, they have layer normalization rather than batch normalization. Batch normalization finds the means among different samples, and layer normalization emphasizes the means within the sample. It is suitable for sequential tasks because the sample have a high inner relationship, but very weak batch-wise relationship. I used RMSnorm since it is slightly faster and make the loss lower (Zhang, et al.).&lt;/p&gt;
&lt;h1 id=&#34;4-experiment&#34;&gt;4. Experiment&lt;/h1&gt;
&lt;p&gt;The training process uses cross-entropy loss, and Adam optimizer with a learning rate of 1e-4. The learning rate will decrease under the control of cosine annealing, which will have a cosine curve that the half period is 20000 iterations. With limited computation power, it is slow to train a model with 28 million parameters, and hard to tweak the hyper-parameters and build an intuition. Other than the slow loss decreasing, in the first round, the low rank model had nan loss after 9100 steps. To solve this problem, I revise the model and restart the training for low rank model.&lt;/p&gt;
&lt;p&gt;It is highly possible that the nan loss caused by some results of SoftMax become 0. The loss function is cross-entropy loss, and in its calculation, there is ln(p), so if p = 0, then ln(p) = -inf. By backpropagation, the invalid value contaminates the whole model. Exploding gradients may cause this problem. Also, I trained the model using BFloat16, which has the same range as float32 but less precision, hence it may severer the problem. First I lowered the learning rate to 1e-5, then use a gradient clip to constrain its value, and set a loss clamp to prevent any infinite loss. Moreover, the low rank model has more linear layers inside, so I add one more layer-normalization in every MLP after the attention layer.&lt;/p&gt;
&lt;p&gt;Although this fixes the nan problem, the loss function does not show a sign of decreasing. At first, I believe the learning rate is still too large so the gradients go overhead, and the batch size should be larger to smooth the loss, but hours later the model still has a similar loss, I realize that other people successfully train larger models with the learning rate of 1e-4, which means I should use some value higher than that. In the rest of the experiment, I use 2e-4 for the baseline model, and 1.5e-4 for the low rank model. Also, the cosine annealing decay period is set to 120000 iterations, so the learning rate can stay high for a long enough time.&lt;/p&gt;
&lt;div style=&#34;display: flex; justify-content: center; align-items: center; flex-wrap: wrap; gap: 20px;&#34;&gt;
&lt;img src=&#34;fig1.png&#34; width=&#34;400&#34; /&gt;
&lt;img src=&#34;fig2.png&#34; width=&#34;400&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;The baseline model is trained with 128600 batches, and the low rank model is trained with 114500 batches, which is more than 12 billion tokens. However, both models fail to generate any meaningful sentence.&lt;/p&gt;
&lt;p&gt;The first peak for those two loss curves is due to that I used label smooth for the loss function. It lowers the probability of the ground truth label; without this option, the ground truth will be a one-hot encoding. It is good for generalization, but if I have enough data and many class labels, the model is hard to overfit. That is why I turn it off after about 10000 iterations. After this peak, because I rise the learning rate so there is another peak.&lt;/p&gt;
&lt;p&gt;The baseline loss curve shows a promising decreasing trend, especially for the evaluation curve; but the low rank&amp;rsquo;s loss curve does not really drop. Since this experiment does not reach its end, I cannot assert that the low-rank method does not work for the training of a relatively small language model. Based on the loss curves, it is clear that the baseline model is much more efficient in training than the low rank model, and has more possibility to have a good performance. One potential justification is that the low rank model is deeper and wider, even if its decoder layers have slightly fewer parameters than the baseline modelâ€™s. Another justification is that text generation is already complex enough for this size of model, so the low rank property only applies to the over-parameterized model.&lt;/p&gt;
&lt;h1 id=&#34;5-conclusion&#34;&gt;5. Conclusion&lt;/h1&gt;
&lt;p&gt;The tradeoff of rank and dimension does not work with a relatively small language model. After about 3 epochs of data, both the baseline and low rank model have not finished training, but the trend shows that the baseline model still can lower its loss in further training, but seems like the low rank model reaches its limitation. Although the experiment is unsuccessful and the hypothesis is falsified, this project establishes my confidence in training large models, and let me realize that model architectures, datasets, training methods, and computing power are the keys to success in machine learning.&lt;/p&gt;
&lt;h1 id=&#34;6-reference&#34;&gt;6. Reference&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. &lt;em&gt;Attention Is All You Need&lt;/em&gt;. &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1706.03762&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/1706.03762&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Biao Zhang, Rico Sennrich. Root Mean Square Layer Normalization. &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1910.07467&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/1910.07467&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chunyuan Li, Heerad Farkhoor, Rosanne Liu, Jason Yosinski. Measuring the Intrinsic Dimension of Objective Landscapes. &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1804.08838&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/1804.08838&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chinese-LLaMA-Alpaca &lt;a class=&#34;link&#34; href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ymcui/Chinese-LLaMA-Alpaca&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;nanoGPT &lt;a class=&#34;link&#34; href=&#34;https://github.com/karpathy/nanoGPT&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/karpathy/nanoGPT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding. &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2104.09864&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/2104.09864&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models. &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2302.13971&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/2302.13971&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ronen Eldan, Yuanzhi Li. &lt;em&gt;TinyStories: How Small Can Language Models Be and Still Speak Coherent English?&lt;/em&gt; &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2305.07759&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/2305.07759&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
