<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>GPT on XFious</title>
        <link>https://example.org/tags/gpt/</link>
        <description>Recent content in GPT on XFious</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <managingEditor>xfious@outlook.com (XFious)</managingEditor>
        <webMaster>xfious@outlook.com (XFious)</webMaster>
        <lastBuildDate>Mon, 13 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://example.org/tags/gpt/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>preliminary experiment for LM distillation and pretraining</title>
        <link>https://example.org/project/dearth-tiny/</link>
        <pubDate>Mon, 13 Nov 2023 00:00:00 +0000</pubDate>
        <author>xfious@outlook.com (XFious)</author>
        <guid>https://example.org/project/dearth-tiny/</guid>
        <description>&lt;img src="https://example.org/project/dearth-tiny/img.png" alt="Featured image of post preliminary experiment for LM distillation and pretraining" /&gt;&lt;h1 id=&#34;language-model-on-tinystories&#34;&gt;Language Model on TinyStories&lt;/h1&gt;
&lt;iframe src=&#34;https://xfious-dearth-tiny.hf.space&#34; style=&#34;width: calc(70vw); height: calc(100vh - 100px); margin: 0px&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;Online demo: &lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/spaces/XFious/dearth-tiny&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://huggingface.co/spaces/XFious/dearth-tiny&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;about-this-project&#34;&gt;About this project&lt;/h1&gt;
&lt;h2 id=&#34;what-is-this-project&#34;&gt;What is this project?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This is a preliminary experiment for pretraining language model, and using distillation to accelerate training and improve performance. The experiment is to verify the effectiveness of the following method:
&lt;ul&gt;
&lt;li&gt;DeepNet (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2203.00555.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2203.00555.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Distillation framework, and corresponding loss function (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2002.10957.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2002.10957.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;New Optimizer Sophia (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2305.14342.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2305.14342.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;front window and attention window, LM_infinite (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2308.16137.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2308.16137.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;other hyperparameters of the training process.&lt;/li&gt;
&lt;li&gt;using MSE loss for soft label&lt;/li&gt;
&lt;li&gt;group-query attention (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2305.13245.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2305.13245.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;computation budget (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2203.15556.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2203.15556.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dearth-tiny is a LM trained based on TinyStories. This model can write short stories with children&amp;rsquo;s level vocabulary; it is not used for instruction QA.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-the-significance-and-purpose-of-this-project&#34;&gt;What is the significance and purpose of this project?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Obtain smaller models through distillation that can be run on CPU and mobile devices&lt;/li&gt;
&lt;li&gt;Compared with pruning methods like Sheared LLaMA, distillation allows more flexible model structure&lt;/li&gt;
&lt;li&gt;Compared with direct training, distillation may improve the effect and accelerate training&lt;/li&gt;
&lt;li&gt;Make the model deeper and have more layers, improving the effect to a certain extent&lt;/li&gt;
&lt;li&gt;Trying to make the model handle extremely long sequences&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-the-limitation-of-this-project&#34;&gt;What is the limitation of this project?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Due to the limitation of training data, this model can only generate short stories using very simple words, which means most language is out of distribution.&lt;/li&gt;
&lt;li&gt;The purpose of the TinyStories dataset is to serve as a sanity check, verifying the effect of the model and training process; the scope and knowledge of the data are very limited, so it does not require a large number of parameters to show a good performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;the-distillation-and-training-process&#34;&gt;The distillation and training process&lt;/h1&gt;
&lt;h2 id=&#34;about-the-model-structure&#34;&gt;About the model structure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;DeepNet: Amplify the residual connection so that the gradient of the layer close to the input will not be too small; allow a deeper model structure to improve the effect to a certain extent;&lt;/li&gt;
&lt;li&gt;LM_infinite: using the front attention window to make later tokens can attend to the front token with enough weights, preventing the out-of-distribution problem.&lt;/li&gt;
&lt;li&gt;Mistral: using attention window to solve long sequence problem (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2310.06825.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2310.06825.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Adding rotary position embedding to query and key vectors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-to-distill-and-train-the-model&#34;&gt;How to distill and train the model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;One training objective is to imitate the attention map and value, and the other is to imitate the logits.&lt;/li&gt;
&lt;li&gt;Using MSE loss for soft label and comparing logits between teacher and student, because according to the profiler&amp;rsquo;s result, softmax required by KL divergence is very time-consuming.&lt;/li&gt;
&lt;li&gt;Seqence length = 256, batch_size for distillation = 200, batch_size for training = 800&lt;/li&gt;
&lt;li&gt;2k steps for distillation with 300 steps warmup, 2k steps for training.&lt;/li&gt;
&lt;li&gt;Using Sophia optimizer, peak lr = 5e-4, beta1 = 0.9, beta2 = 0.99, weight decay = 0.2.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-the-error-that-has-been-corrected&#34;&gt;What is the error that has been corrected?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In the previous project about low-rank LM, the loss function is wrong, because it only checks the logits of the last token, causing a very slow and unstable training process. It is unreasonable to only check the last token&amp;rsquo;s logits, because the model is trained to predict the next token, not the last token, so every output is usable to estimate the loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.org/project/dearth-tiny/loss-ts100-re2.png&#34;
	width=&#34;977&#34;
	height=&#34;990&#34;
	srcset=&#34;https://example.org/project/dearth-tiny/loss-ts100-re2_hufd0b3d3930ee9759a967dbdea3dadaaa_55644_480x0_resize_box_3.png 480w, https://example.org/project/dearth-tiny/loss-ts100-re2_hufd0b3d3930ee9759a967dbdea3dadaaa_55644_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;98&#34;
		data-flex-basis=&#34;236px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: shows each loss component. After 2000 steps, loss_soft and loss_mimic are not used in the training process.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.org/project/dearth-tiny/lr-ts100-re2.png&#34;
	width=&#34;573&#34;
	height=&#34;413&#34;
	srcset=&#34;https://example.org/project/dearth-tiny/lr-ts100-re2_hu1efa4cf25de4292373e54e67f5efcd5b_22492_480x0_resize_box_3.png 480w, https://example.org/project/dearth-tiny/lr-ts100-re2_hu1efa4cf25de4292373e54e67f5efcd5b_22492_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;138&#34;
		data-flex-basis=&#34;332px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2: learning rate for distillation and training.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;work-in-progress&#34;&gt;Work in progress&lt;/h1&gt;
&lt;h2 id=&#34;what-can-be-done-to-improve-the-effect-why-is-the-effect-not-good&#34;&gt;What can be done to improve the effect? Why is the effect not good?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The current PPL of the student model is 1.7, and the teacher PPL is 0.9.&lt;/li&gt;
&lt;li&gt;Need more training steps&lt;/li&gt;
&lt;li&gt;Need to experiment with more suitable learning rate with the new optimizer&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;potential-problems&#34;&gt;Potential problems&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Compared with the teacher model, the student model has fewer parameters. If the student spend too much capacity on fitting the internal structure, it may be difficult to fit the hard label loss (the loss for the next token).&lt;/li&gt;
&lt;li&gt;Whether the distillation process, that is, learning the internal structure, can replace the fact that the model has experienced a large number of tokens; that is to say, is the ability of the large language model derived from the acquisition of attention map, or from the subtle internal representation that must learn from diverse and numerous training data?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-the-work-in-progress&#34;&gt;What is the work in progress?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Adjust the learning rate so that the training process can be intervened manually to adjust; avoid interrupting the training.&lt;/li&gt;
&lt;li&gt;Distill 7B into 1B, and then compare it with other 1B models&lt;/li&gt;
&lt;li&gt;Instruction fine-tuning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-the-future-work&#34;&gt;What is the future work?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;PPO for alignment.&lt;/li&gt;
&lt;li&gt;Memory-augmented model, adding memory to improve the effect of the model; the memory stored in the database can make the model more controllable and may reduce the illusion caused by the memory stored in the parameters.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Are Small Language Models Low-rank?</title>
        <link>https://example.org/project/low-rank-lm/</link>
        <pubDate>Thu, 15 Jun 2023 00:00:00 +0000</pubDate>
        <author>xfious@outlook.com (XFious)</author>
        <guid>https://example.org/project/low-rank-lm/</guid>
        <description>&lt;img src="https://example.org/project/low-rank-lm/head.png" alt="Featured image of post Are Small Language Models Low-rank?" /&gt;&lt;h1 id=&#34;are-small-language-models-low-rank&#34;&gt;Are Small Language Models Low-rank?&lt;/h1&gt;
&lt;h2 id=&#34;explore-causal-lm-training-and-increase-hidden-dimension-with-low-rank-matrices&#34;&gt;Explore causal LM training and increase hidden dimension with low-rank matrices.&lt;/h2&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Over-parameterized language models are low rank intrinsically. In this project, I trained 2 causal language models with 28M parameters each, such that one is the baseline and another uses low-rank weights but has higher hidden dimensions, and compare their training speed and accuracy. Although they are trained with more than 12 billion tokens, both models fail to finish training. According to the loss curve on hand, the baseline model trounces the low-rank model in speed and performance.
GITHUB: &lt;a class=&#34;link&#34; href=&#34;https://github.com/XiaonanFu-ucsd/COGS185-final-project&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/XiaonanFu-ucsd/COGS185-final-project&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;Large language models show their power in few-shot learning ability, and the potential to solve most downstream tasks. Just like their name, they are large and expensive to train due to the amount of parameters, and hard to infer on a local machine. If there is some method to make small enough models have decent text generate ability, then a personal device can do local inference in a private and efficient manner. Given that the causal language model with high hidden dimension usually performs better, I explored to use low rank matrices product to simulate a larger model with the same amount of parameters. To compare the overall performance of the normal model and low rank model in terms of training efficiency and accuracy, I trained two models with 28M parameters each, with multiple datasets over 7GB of text. In the training process, I faced several challenges and tried to adjust multiple hyper-parameters to speed up training, such as sequence length, batch size, loss function, and learning rate. Although the low rank model does not work in the limited training time, the exploration of training a language model is still a valuable experience.&lt;/p&gt;
&lt;h1 id=&#34;2-related-work&#34;&gt;2. Related Work&lt;/h1&gt;
&lt;h2 id=&#34;21-attention-in-transformers&#34;&gt;2.1 Attention in Transformers&lt;/h2&gt;
&lt;p&gt;Each Attention layer in transformers uses three matrices to map every token embedding to be query, key, and value vectors. The dot product of the query and key vectors, decides the weight of the corresponding value vectors after the SoftMax operation. After this process, one token embedding obtains other embedding’s value with different weights, so the model has the ability to learn the sequential relationship across different tokens.&lt;/p&gt;
&lt;p&gt;The positional encoding is also critical for the attention mechanism, since the token embedding itself does not contain the structural information, it should be added in every attention layer. There are many ways to add positional encoding, and in the paper &lt;em&gt;Attention Is All You Need&lt;/em&gt;, it presents sinusoidal positional encoding and learned positional encoding, and they find that they have similar performance, but the sinusoidal version may be better at extrapolation.&lt;/p&gt;
&lt;h2 id=&#34;22-causal-language-model&#34;&gt;2.2 Causal Language Model&lt;/h2&gt;
&lt;p&gt;Causal language models use masked attention to restrict every token can only reference the previous token, so they are more relevant and fit the text generation task. In the implementation, they add a mask with some negative infinity to the dot product result, then the number will be zero after SoftMax. Although they are decoder-only structure, the decoder just contains one attention layer and one multilayer feed-forward network.&lt;/p&gt;
&lt;p&gt;From a holistic perspective to describe the text generation process, a tokenizer encode the prompt to a sequence of token, which can be understood as an integer index in a dictionary. The model uses a lookup table to find the embedding for each token, and send embeddings to decoder layers. In the end, embeddings multiply with a large matric to produce the logit, then use SoftMax to find which logit has the largest probability. It is a multiclass classification task.&lt;/p&gt;
&lt;h2 id=&#34;23-low-rank-adaptation&#34;&gt;2.3 Low-rank adaptation&lt;/h2&gt;
&lt;p&gt;Low-rank adaptation is a common fine-turn method to make a base model have a better performance in a downstream task. Inspired by the finding that a large model may not fully utilize their express ability, and those weights have low rank intrinsically, Edward Hu, et al. hypothesize that the amount of weight change also has this low rank property. Based on LORA, fine-turning only need a small amount of parameter to adjust a large model; a larger matrix is the product of two smaller matrices with a much lower rank.&lt;/p&gt;
&lt;img src=&#34;img1.png&#34; width=&#34;200&#34; height=&#34;200&#34; /&gt;
&lt;h2 id=&#34;24-tinystories-dataset&#34;&gt;2.4 TinyStories Dataset&lt;/h2&gt;
&lt;p&gt;Ronen Eldan, et al. build a small enough dataset that covers diverse vocabulary, facts, and reasoning. They used GPT4 prompted with randomly combined none and verb, to produce millions of stories. Using this dataset, their model with 28 million parameters performance better than the GPT2-large with 774 million parameters in terms of story continuation.&lt;/p&gt;
&lt;h1 id=&#34;3-method&#34;&gt;3. Method&lt;/h1&gt;
&lt;p&gt;The datasets are mainly composed of the TinyStories and TinyStories Instruction datasets. I also use Alpaca Chinese, Dolly 15K, Lightnovels_EN, Chinese Metaphors datasets as supplementary, and allow the model learn bilingual text generation. About 2/3 of the data comes from TinyStories datasets.&lt;/p&gt;
&lt;p&gt;Since the datasets are large, I designed a multi-processing dataloader, which can randomly pick which dataset to sample from with the given proportion, and does not need to load all the datasets into the memory. The dataloader needs to tokenize the text while training. If one independent sample text is too long, it will randomly select part of it to fit the specified token sequence length. If the text is too short, it will pad the text with a padding token at the front. The sequence length used in this project is 1024, such that the model needs to predict the last token. By default, the tokenizer will put BOS at the beginning and EOS at the end. To prevent most rows end in EOS, the dataloader will randomly crop the row; it will also increase the robustness of the model. The batch size is 120.&lt;/p&gt;
&lt;p&gt;I used the tokenizer from the Chinese-LLaMA-Alpaca project, which extends the LLaMA vocabulary with Chinese words. The vocabulary size of this tokenizer is 49954.&lt;/p&gt;
&lt;p&gt;The baseline model references the implementation of nanoGPT and Meta’s LLAMA. It has 4 layers with a hidden size of 768 and 12 heads, in a total of 28M parameters, except for the final linear layer which converts embedding to logit. Every attention layer needs positional information. Rather than the methods presented in Attention Is All You Need, this model used rotary positional encoding, which does not add to the token embedding, but element-wisely multiplies to the matrix x_q and x_k. This encoding has decreasing sinusoidal frequency for larger dimensions (Su, et al.). Flash attention is used to speed up attention computation. After an attention layer, there is a two-layer FFN, for each is 768*3072; between them is a SiLU activation function.&lt;/p&gt;
&lt;p&gt;The low rank model has the same structure, but uses low rank weights matrices to achieve a higher hidden dimension, which is 1152, and has 18 heads.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;wq&lt;/th&gt;
&lt;th&gt;wk&lt;/th&gt;
&lt;th&gt;wv&lt;/th&gt;
&lt;th&gt;wo&lt;/th&gt;
&lt;th&gt;Linear&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Baseline&lt;/td&gt;
&lt;td&gt;768*768&lt;/td&gt;
&lt;td&gt;768*768&lt;/td&gt;
&lt;td&gt;768*768&lt;/td&gt;
&lt;td&gt;768*768&lt;/td&gt;
&lt;td&gt;768*3072 * 2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Low-rank&lt;/td&gt;
&lt;td&gt;1152*256 * 2&lt;/td&gt;
&lt;td&gt;1152*256 * 2&lt;/td&gt;
&lt;td&gt;1152*256 * 2&lt;/td&gt;
&lt;td&gt;1152*256 * 2&lt;/td&gt;
&lt;td&gt;(1152*512 + 512*2880) * 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Table 1: comparing the dimension of models&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For both models, they have layer normalization rather than batch normalization. Batch normalization finds the means among different samples, and layer normalization emphasizes the means within the sample. It is suitable for sequential tasks because the sample have a high inner relationship, but very weak batch-wise relationship. I used RMSnorm since it is slightly faster and make the loss lower (Zhang, et al.).&lt;/p&gt;
&lt;h1 id=&#34;4-experiment&#34;&gt;4. Experiment&lt;/h1&gt;
&lt;p&gt;The training process uses cross-entropy loss, and Adam optimizer with a learning rate of 1e-4. The learning rate will decrease under the control of cosine annealing, which will have a cosine curve that the half period is 20000 iterations. With limited computation power, it is slow to train a model with 28 million parameters, and hard to tweak the hyper-parameters and build an intuition. Other than the slow loss decreasing, in the first round, the low rank model had nan loss after 9100 steps. To solve this problem, I revise the model and restart the training for low rank model.&lt;/p&gt;
&lt;p&gt;It is highly possible that the nan loss caused by some results of SoftMax become 0. The loss function is cross-entropy loss, and in its calculation, there is ln(p), so if p = 0, then ln(p) = -inf. By backpropagation, the invalid value contaminates the whole model. Exploding gradients may cause this problem. Also, I trained the model using BFloat16, which has the same range as float32 but less precision, hence it may severer the problem. First I lowered the learning rate to 1e-5, then use a gradient clip to constrain its value, and set a loss clamp to prevent any infinite loss. Moreover, the low rank model has more linear layers inside, so I add one more layer-normalization in every MLP after the attention layer.&lt;/p&gt;
&lt;p&gt;Although this fixes the nan problem, the loss function does not show a sign of decreasing. At first, I believe the learning rate is still too large so the gradients go overhead, and the batch size should be larger to smooth the loss, but hours later the model still has a similar loss, I realize that other people successfully train larger models with the learning rate of 1e-4, which means I should use some value higher than that. In the rest of the experiment, I use 2e-4 for the baseline model, and 1.5e-4 for the low rank model. Also, the cosine annealing decay period is set to 120000 iterations, so the learning rate can stay high for a long enough time.&lt;/p&gt;
&lt;div style=&#34;display: flex; justify-content: center; align-items: center; flex-wrap: wrap; gap: 20px;&#34;&gt;
&lt;img src=&#34;fig1.png&#34; width=&#34;400&#34; /&gt;
&lt;img src=&#34;fig2.png&#34; width=&#34;400&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;The baseline model is trained with 128600 batches, and the low rank model is trained with 114500 batches, which is more than 12 billion tokens. However, both models fail to generate any meaningful sentence.&lt;/p&gt;
&lt;p&gt;The first peak for those two loss curves is due to that I used label smooth for the loss function. It lowers the probability of the ground truth label; without this option, the ground truth will be a one-hot encoding. It is good for generalization, but if I have enough data and many class labels, the model is hard to overfit. That is why I turn it off after about 10000 iterations. After this peak, because I rise the learning rate so there is another peak.&lt;/p&gt;
&lt;p&gt;The baseline loss curve shows a promising decreasing trend, especially for the evaluation curve; but the low rank&amp;rsquo;s loss curve does not really drop. Since this experiment does not reach its end, I cannot assert that the low-rank method does not work for the training of a relatively small language model. Based on the loss curves, it is clear that the baseline model is much more efficient in training than the low rank model, and has more possibility to have a good performance. One potential justification is that the low rank model is deeper and wider, even if its decoder layers have slightly fewer parameters than the baseline model’s. Another justification is that text generation is already complex enough for this size of model, so the low rank property only applies to the over-parameterized model.&lt;/p&gt;
&lt;h1 id=&#34;5-conclusion&#34;&gt;5. Conclusion&lt;/h1&gt;
&lt;p&gt;The tradeoff of rank and dimension does not work with a relatively small language model. After about 3 epochs of data, both the baseline and low rank model have not finished training, but the trend shows that the baseline model still can lower its loss in further training, but seems like the low rank model reaches its limitation. Although the experiment is unsuccessful and the hypothesis is falsified, this project establishes my confidence in training large models, and let me realize that model architectures, datasets, training methods, and computing power are the keys to success in machine learning.&lt;/p&gt;
&lt;h1 id=&#34;6-reference&#34;&gt;6. Reference&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. &lt;em&gt;Attention Is All You Need&lt;/em&gt;. &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1706.03762&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/1706.03762&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Biao Zhang, Rico Sennrich. Root Mean Square Layer Normalization. &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1910.07467&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/1910.07467&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chunyuan Li, Heerad Farkhoor, Rosanne Liu, Jason Yosinski. Measuring the Intrinsic Dimension of Objective Landscapes. &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1804.08838&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/1804.08838&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chinese-LLaMA-Alpaca &lt;a class=&#34;link&#34; href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ymcui/Chinese-LLaMA-Alpaca&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;nanoGPT &lt;a class=&#34;link&#34; href=&#34;https://github.com/karpathy/nanoGPT&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/karpathy/nanoGPT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding. &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2104.09864&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/2104.09864&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models. &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2302.13971&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/2302.13971&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ronen Eldan, Yuanzhi Li. &lt;em&gt;TinyStories: How Small Can Language Models Be and Still Speak Coherent English?&lt;/em&gt; &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2305.07759&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/2305.07759&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
